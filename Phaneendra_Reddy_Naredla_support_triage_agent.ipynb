{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Install Dependencies"
      ],
      "metadata": {
        "id": "CuUD6bSOiZHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8XaS1InH-Ete"
      },
      "outputs": [],
      "source": [
        "# Colab cell 1: installed dependencies\n",
        "!pip install --quiet fastapi uvicorn[standard] pydantic \"python-dotenv\" pytest requests matplotlib seaborn scikit-learn\n",
        "\n",
        "!pip install --quiet pyngrok openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All required packages like FastAPI, Uvicorn, Flask, requests, and OpenAI are installed.\n",
        "These libraries enable backend API, UI, LLM processing, and testing."
      ],
      "metadata": {
        "id": "fi5vulaGib4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Create Project Structure"
      ],
      "metadata": {
        "id": "vnJSwaccif3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating project structure and files\n",
        "import os, json, textwrap\n",
        "\n",
        "root = \"support-triage-agent\"\n",
        "os.makedirs(root, exist_ok=True)\n",
        "\n",
        "files = {\n",
        "    # app files\n",
        "    os.path.join(root, \"app\", \"main.py\"): textwrap.dedent(\"\"\"\\\n",
        "        from fastapi import FastAPI, HTTPException\n",
        "        from app.models import TriageRequest, TriageResponse\n",
        "        from app.agent.triage_agent import TriageAgent\n",
        "        import os\n",
        "\n",
        "        app = FastAPI(title=\"Support Triage Agent\")\n",
        "\n",
        "        # Use real OpenAI if OPENAI_API_KEY provided, otherwise use mock LLM\n",
        "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "        agent = TriageAgent(use_openai=bool(OPENAI_API_KEY))\n",
        "\n",
        "        @app.post(\"/triage\", response_model=TriageResponse)\n",
        "        async def triage_endpoint(req: TriageRequest):\n",
        "            desc = req.description.strip()\n",
        "            if not desc:\n",
        "                raise HTTPException(status_code=422, detail=\"description cannot be empty\")\n",
        "            return agent.handle_ticket(desc)\n",
        "    \"\"\"),\n",
        "\n",
        "    os.path.join(root, \"app\", \"models.py\"): textwrap.dedent(\"\"\"\\\n",
        "        from pydantic import BaseModel\n",
        "        from typing import List, Dict, Any\n",
        "\n",
        "        class TriageRequest(BaseModel):\n",
        "            description: str\n",
        "\n",
        "        class KBItem(BaseModel):\n",
        "            id: str\n",
        "            title: str\n",
        "            category: str\n",
        "            symptoms: List[str]\n",
        "            recommended_action: str\n",
        "\n",
        "        class TriageResponse(BaseModel):\n",
        "            summary: str\n",
        "            category: str\n",
        "            severity: str\n",
        "            likely: str\n",
        "            related_issues: List[KBItem]\n",
        "            suggested_action: str\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/triage_agent.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"triage_agent.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import os\n",
        "        from typing import List, Dict, Any\n",
        "        from app.agent.kb_search import KBSearchTool\n",
        "        from app.agent.llm_client import LLMClient\n",
        "\n",
        "        class TriageAgent:\n",
        "            def __init__(self, kb_path=None, use_openai=False):\n",
        "                # kb_path optional; default uses package data file path\n",
        "                cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "                default_kb = os.path.join(cwd, \"..\", \"..\", \"data\", \"kb.json\")\n",
        "                self.kb_path = kb_path or default_kb\n",
        "                self.kb_tool = KBSearchTool(self.kb_path)\n",
        "                self.llm = LLMClient(use_openai=use_openai)\n",
        "\n",
        "            def handle_ticket(self, description: str) -> Dict[str, Any]:\n",
        "                summary = self.llm.summarize(description)\n",
        "                category = self.llm.classify_category(description)\n",
        "                severity = self.llm.classify_severity(description)\n",
        "\n",
        "                related = self.kb_tool.search(description, top_k=3)\n",
        "                known_issue = len(related) > 0\n",
        "\n",
        "                if known_issue:\n",
        "                    action = \"Attach KB article and respond to user\"\n",
        "                else:\n",
        "                    if severity in (\"Critical\", \"High\"):\n",
        "                        action = \"Escalate to backend on-call\"\n",
        "                    else:\n",
        "                        action = \"Request logs/screenshots from customer\"\n",
        "\n",
        "                return {\n",
        "                    \"summary\": summary,\n",
        "                    \"category\": category,\n",
        "                    \"severity\": severity,\n",
        "                    \"likely\": \"known_issue\" if known_issue else \"new_issue\",\n",
        "                    \"related_issues\": related,\n",
        "                    \"suggested_action\": action\n",
        "                }\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/llm_client.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"llm_client.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import os\n",
        "        from typing import Optional\n",
        "        try:\n",
        "            import openai\n",
        "        except Exception:\n",
        "            openai = None\n",
        "\n",
        "        class LLMClient:\n",
        "            def __init__(self, use_openai: bool = False, model_name: str = \"gpt-3.5-turbo\"):\n",
        "                self.use_openai = use_openai and (openai is not None)\n",
        "                self.model_name = model_name\n",
        "                self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "                if self.use_openai and not self.api_key:\n",
        "                    # fallback to mock if key missing\n",
        "                    self.use_openai = False\n",
        "\n",
        "                if self.use_openai:\n",
        "                    openai.api_key = self.api_key\n",
        "\n",
        "            def summarize(self, text: str) -> str:\n",
        "                if self.use_openai:\n",
        "                    # inexpensive, short prompt - optional (calls OpenAI)\n",
        "                    prompt = f\"Summarize the following support ticket in one short sentence:\\\\n\\\\n{text}\"\n",
        "                    try:\n",
        "                        resp = openai.ChatCompletion.create(\n",
        "                            model=self.model_name,\n",
        "                            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                            max_tokens=50,\n",
        "                            temperature=0.0\n",
        "                        )\n",
        "                        return resp.choices[0].message.content.strip()\n",
        "                    except Exception as e:\n",
        "                        # graceful fallback to heuristic\n",
        "                        print(\\\"[LLM WARNING] OpenAI call failed, falling back to heuristic:\\\", e)\n",
        "                # heuristic summarization\n",
        "                sentences = [s.strip() for s in text.replace('\\\\n',' ').split('.') if s.strip()]\n",
        "                return sentences[0][:250] if sentences else text[:250]\n",
        "\n",
        "            def classify_severity(self, text: str) -> str:\n",
        "                # simple heuristics; you can replace with LLM classification if desired\n",
        "                t = text.lower()\n",
        "                if any(k in t for k in [\\\"data loss\\\",\\\"payment\\\",\\\"security\\\",\\\"critical\\\",\\\"outage\\\",\\\"down\\\",\\\"cannot login\\\"]):\n",
        "                    return \\\"Critical\\\"\n",
        "                if any(k in t for k in [\\\"fails\\\",\\\"failure\\\",\\\"error\\\",\\\"500\\\",\\\"502\\\",\\\"503\\\",\\\"unavailable\\\",\\\"crash\\\"]):\n",
        "                    return \\\"High\\\"\n",
        "                if any(k in t for k in [\\\"slow\\\",\\\"latency\\\",\\\"delay\\\",\\\"timeout\\\"]):\n",
        "                    return \\\"Medium\\\"\n",
        "                return \\\"Low\\\"\n",
        "\n",
        "            def classify_category(self, text: str) -> str:\n",
        "                t = text.lower()\n",
        "                if any(k in t for k in [\\\"billing\\\",\\\"invoice\\\",\\\"payment\\\",\\\"refund\\\"]):\n",
        "                    return \\\"Billing\\\"\n",
        "                if any(k in t for k in [\\\"login\\\",\\\"signin\\\",\\\"password\\\",\\\"2fa\\\"]):\n",
        "                    return \\\"Login\\\"\n",
        "                if any(k in t for k in [\\\"error\\\",\\\"exception\\\",\\\"stacktrace\\\",\\\"crash\\\"]):\n",
        "                    return \\\"Bug\\\"\n",
        "                if any(k in t for k in [\\\"slow\\\",\\\"performance\\\",\\\"latency\\\",\\\"timeout\\\"]):\n",
        "                    return \\\"Performance\\\"\n",
        "                return \\\"Question/How-To\\\"\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/kb_search.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"kb_search.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import json\n",
        "        import os\n",
        "        from typing import List, Dict, Any\n",
        "\n",
        "        class KBSearchTool:\n",
        "            def __init__(self, kb_path: str):\n",
        "                if not os.path.exists(kb_path):\n",
        "                    raise FileNotFoundError(f\\\"KB file not found: {kb_path}\\\")\n",
        "                with open(kb_path, 'r', encoding='utf-8') as f:\n",
        "                    self.kb = json.load(f)\n",
        "\n",
        "            def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "                q = query.lower()\n",
        "                scored = []\n",
        "                for entry in self.kb:\n",
        "                    text = ' '.join([entry.get('title',''), ' '.join(entry.get('symptoms',[])), entry.get('category','')]).lower()\n",
        "                    # keyword overlap score\n",
        "                    score = sum(1 for tok in q.split() if tok in text)\n",
        "                    scored.append((score, entry))\n",
        "                # filter zeros and sort\n",
        "                matches = [e for e in scored if e[0] > 0]\n",
        "                matches.sort(key=lambda x: x[0], reverse=True)\n",
        "                return [m[1] for m in matches[:top_k]]\n",
        "    \"\"\"),\n",
        "\n",
        "    # data/kb.json (10-15 sample items)\n",
        "    os.path.join(root, \"data\", \"kb.json\"): json.dumps([\n",
        "        {\"id\":\"KB-001\",\"title\":\"Checkout error 500 on mobile\",\"category\":\"Bug\",\"symptoms\":[\"500 error\",\"mobile\",\"checkout\"],\"recommended_action\":\"Escalate to payments engineer; collect server trace\"},\n",
        "        {\"id\":\"KB-002\",\"title\":\"Password reset emails not delivered\",\"category\":\"Login\",\"symptoms\":[\"password reset\",\"email not received\",\"ses\"],\"recommended_action\":\"Check email provider logs and bounce metrics\"},\n",
        "        {\"id\":\"KB-003\",\"title\":\"Slow dashboard load after login\",\"category\":\"Performance\",\"symptoms\":[\"slow\",\"dashboard\",\"login\",\"latency\"],\"recommended_action\":\"Investigate DB slow queries and caching layer\"},\n",
        "        {\"id\":\"KB-004\",\"title\":\"Payment declined for specific card type\",\"category\":\"Billing\",\"symptoms\":[\"payment declined\",\"card\",\"visa\",\"amex\"],\"recommended_action\":\"Ask for card BIN and escalate to payments gateway\"},\n",
        "        {\"id\":\"KB-005\",\"title\":\"2FA codes delayed or missing\",\"category\":\"Login\",\"symptoms\":[\"2fa\",\"sms\",\"codes\",\"delayed\"],\"recommended_action\":\"Validate SMS provider status and retry logic\"},\n",
        "        {\"id\":\"KB-006\",\"title\":\"Mobile app crashes at checkout\",\"category\":\"Bug\",\"symptoms\":[\"crash\",\"mobile\",\"checkout\",\"stacktrace\"],\"recommended_action\":\"Request crash logs and device model; escalate to mobile team\"},\n",
        "        {\"id\":\"KB-007\",\"title\":\"Invoice amounts mismatch\",\"category\":\"Billing\",\"symptoms\":[\"invoice\",\"amount mismatch\",\"billing\"],\"recommended_action\":\"Sync ledger and run reconciliation job\"},\n",
        "        {\"id\":\"KB-008\",\"title\":\"Search returns incomplete results\",\"category\":\"Bug\",\"symptoms\":[\"search\",\"missing results\",\"index\"],\"recommended_action\":\"Check indexing job and reindex if necessary\"},\n",
        "        {\"id\":\"KB-009\",\"title\":\"API rate limit errors 429\",\"category\":\"Performance\",\"symptoms\":[\"429\",\"rate limit\",\"api\"],\"recommended_action\":\"Suggest client-side backoff and review quota settings\"},\n",
        "        {\"id\":\"KB-010\",\"title\":\"Image upload fails for large files\",\"category\":\"Bug\",\"symptoms\":[\"upload\",\"image\",\"size limit\",\"s3\"],\"recommended_action\":\"Increase size limits or compress client images before upload\"},\n",
        "        {\"id\":\"KB-011\",\"title\":\"Refund processing delayed\",\"category\":\"Billing\",\"symptoms\":[\"refund\",\"delayed\",\"payment\"],\"recommended_action\":\"Check payment provider ledger and refund queue\"},\n",
        "        {\"id\":\"KB-012\",\"title\":\"Feature flag not toggling for subset of users\",\"category\":\"Bug\",\"symptoms\":[\"feature flag\",\"toggle\",\"rollout\"],\"recommended_action\":\"Investigate feature service and rollback if necessary\"}\n",
        "    ], indent=2),\n",
        "\n",
        "    # tests/test_api.py\n",
        "    os.path.join(root, \"tests\", \"test_api.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import sys, os, json\n",
        "        sys.path.append(os.path.join(os.getcwd(), \"support-triage-agent\"))\n",
        "        from fastapi.testclient import TestClient\n",
        "        from app.main import app\n",
        "\n",
        "        client = TestClient(app)\n",
        "\n",
        "        def test_triage_endpoint_known_issue():\n",
        "            payload = {\"description\": \"Checkout keeps failing with error 500 on mobile when I try to pay.\"}\n",
        "            r = client.post(\"/triage\", json=payload)\n",
        "            assert r.status_code == 200\n",
        "            j = r.json()\n",
        "            assert \"summary\" in j\n",
        "            assert j[\"category\"] in [\"Bug\",\"Billing\",\"Login\",\"Performance\",\"Question/How-To\"]\n",
        "            assert j[\"severity\"] in [\"Low\",\"Medium\",\"High\",\"Critical\"]\n",
        "\n",
        "        def test_triage_endpoint_empty():\n",
        "            r = client.post(\"/triage\", json={\"description\": \"\"})\n",
        "            assert r.status_code == 422\n",
        "    \"\"\"),\n",
        "\n",
        "    # Dockerfile\n",
        "    os.path.join(root, \"Dockerfile\"): textwrap.dedent(\"\"\"\\\n",
        "        FROM python:3.11-slim\n",
        "        WORKDIR /app\n",
        "        COPY . /app\n",
        "        RUN pip install --no-cache-dir -r requirements.txt\n",
        "        EXPOSE 8000\n",
        "        CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "    \"\"\"),\n",
        "\n",
        "    # requirements.txt\n",
        "    os.path.join(root, \"requirements.txt\"): textwrap.dedent(\"\"\"\\\n",
        "        fastapi\n",
        "        uvicorn[standard]\n",
        "        pydantic\n",
        "        python-dotenv\n",
        "        pytest\n",
        "        requests\n",
        "        openai\n",
        "        matplotlib\n",
        "        seaborn\n",
        "        scikit-learn\n",
        "    \"\"\"),\n",
        "\n",
        "    # README.md\n",
        "    os.path.join(root, \"README.md\"): textwrap.dedent(\"\"\"\\\n",
        "        # Support Triage Agent (Submission)\n",
        "        ## Structure\n",
        "        support-triage-agent/\n",
        "        ├─ app/\n",
        "        │  ├─ main.py\n",
        "        │  ├─ models.py\n",
        "        │  ├─ agent/\n",
        "        │  │  ├─ triage_agent.py\n",
        "        │  │  ├─ llm_client.py\n",
        "        │  │  └─ kb_search.py\n",
        "        ├─ data/\n",
        "        │  └─ kb.json\n",
        "        ├─ tests/\n",
        "        │  └─ test_api.py\n",
        "        ├─ Dockerfile\n",
        "        ├─ requirements.txt\n",
        "\n",
        "        ##Introduction\n",
        "        This project implements a Support Ticket Triage System using FastAPI, a custom LLM-powered agent, a Knowledge Base (KB) search tool, and an optional Flask-based user interface. The goal of this system is to automatically classify incoming customer support tickets, estimate their severity, retrieve related known issues, and recommend the next action for support teams.\n",
        "        The notebook demonstrates the complete workflow: installation of dependencies, creation of project structure, implementation of agent logic, testing with PyTest, OpenAI integration upgrades, and running a web UI with ngrok exposure.\n",
        "        2. Environment Setup & Dependencies\n",
        "        The first part of your notebook installs all required libraries including:\n",
        "        FastAPI + Uvicorn for backend API\n",
        "        Pydantic for request/response models\n",
        "        Requests for API calls\n",
        "        PyTest for testing\n",
        "        OpenAI Python SDK\n",
        "        Flask for UI\n",
        "        pyngrok for exposing local apps\n",
        "        matplotlib, seaborn, scikit-learn (not heavily used, but included)\n",
        "        This ensures the environment is consistent and reproducible.\n",
        "\n",
        "        ## Run (development)\n",
        "        1. Install requirements: `pip install -r requirements.txt`\n",
        "        2. Start server: `uvicorn app.main:app --reload --port 8000`\n",
        "        3. POST to /triage with JSON `{\\\"description\\\": \\\"...\\\"}`\n",
        "\n",
        "        ## OpenAI integration (optional)\n",
        "        Setting an environment variable `OPENAI_API_KEY`. The app will use OpenAI for summarization if the key is present.\n",
        "\n",
        "        ##this is important to notice\n",
        "        - Cleaning separation of concerns, test coverage for API endpoints, optional LLM integration.\n",
        "        - KB stored as JSON for simplicity; replacing with vector DB for production (FAISS/Pinecone).\n",
        "    \"\"\"),\n",
        "}\n",
        "\n",
        "# creating directories and write files\n",
        "for path, content in files.items():\n",
        "    d = os.path.dirname(path)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    # content might already be a str or JSON dumped\n",
        "    if isinstance(content, str):\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "    else:\n",
        "        # already JSON string\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "print(\"Project created at:\", os.path.abspath(root))\n",
        "# listing tree to confirm\n",
        "for root_dir, dirs, filenames in os.walk(\"support-triage-agent\"):\n",
        "    level = root_dir.replace(\"support-triage-agent\", \"\").count(os.sep)\n",
        "    indent = \"  \" * level\n",
        "    print(f\"{indent}{os.path.basename(root_dir)}/\")\n",
        "    for fname in filenames:\n",
        "        print(f\"{indent}  - {fname}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L-cxOCOolLP",
        "outputId": "b5ffdc1c-9fbf-4f00-e4f4-386dd10d45b8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project created at: /content/support-triage-agent/flask_ui/support-triage-agent\n",
            "support-triage-agent/\n",
            "  - Dockerfile\n",
            "  - README.md\n",
            "  - requirements.txt\n",
            "  data/\n",
            "    - kb.json\n",
            "  tests/\n",
            "    - test_api.py\n",
            "  app/\n",
            "    - main.py\n",
            "    - models.py\n",
            "    agent/\n",
            "      - llm_client.py\n",
            "      - kb_search.py\n",
            "      - triage_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating project structure and files\n",
        "import os, json, textwrap\n",
        "\n",
        "root = \"support-triage-agent\"\n",
        "os.makedirs(root, exist_ok=True)\n",
        "\n",
        "files = {\n",
        "    # app files\n",
        "    os.path.join(root, \"app\", \"main.py\"): textwrap.dedent(\"\"\"\\\n",
        "        from fastapi import FastAPI, HTTPException\n",
        "        from app.models import TriageRequest, TriageResponse\n",
        "        from app.agent.triage_agent import TriageAgent\n",
        "        import os\n",
        "\n",
        "        app = FastAPI(title=\"Support Triage Agent\")\n",
        "\n",
        "        # Use real OpenAI if OPENAI_API_KEY provided, otherwise use mock LLM\n",
        "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "        agent = TriageAgent(use_openai=bool(OPENAI_API_KEY))\n",
        "\n",
        "        @app.post(\"/triage\", response_model=TriageResponse)\n",
        "        async def triage_endpoint(req: TriageRequest):\n",
        "            desc = req.description.strip()\n",
        "            if not desc:\n",
        "                raise HTTPException(status_code=422, detail=\"description cannot be empty\")\n",
        "            return agent.handle_ticket(desc)\n",
        "    \"\"\"),\n",
        "\n",
        "    os.path.join(root, \"app\", \"models.py\"): textwrap.dedent(\"\"\"\\\n",
        "        from pydantic import BaseModel\n",
        "        from typing import List, Dict, Any\n",
        "\n",
        "        class TriageRequest(BaseModel):\n",
        "            description: str\n",
        "\n",
        "        class KBItem(BaseModel):\n",
        "            id: str\n",
        "            title: str\n",
        "            category: str\n",
        "            symptoms: List[str]\n",
        "            recommended_action: str\n",
        "\n",
        "        class TriageResponse(BaseModel):\n",
        "            summary: str\n",
        "            category: str\n",
        "            severity: str\n",
        "            likely: str\n",
        "            related_issues: List[KBItem]\n",
        "            suggested_action: str\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/triage_agent.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"triage_agent.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import os\n",
        "        from typing import List, Dict, Any\n",
        "        from app.agent.kb_search import KBSearchTool\n",
        "        from app.agent.llm_client import LLMClient\n",
        "\n",
        "        class TriageAgent:\n",
        "            def __init__(self, kb_path=None, use_openai=False):\n",
        "                # kb_path optional; default uses package data file path\n",
        "                cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "                default_kb = os.path.join(cwd, \"..\", \"..\", \"data\", \"kb.json\")\n",
        "                self.kb_path = kb_path or default_kb\n",
        "                self.kb_tool = KBSearchTool(self.kb_path)\n",
        "                self.llm = LLMClient(use_openai=use_openai)\n",
        "\n",
        "            def handle_ticket(self, description: str) -> Dict[str, Any]:\n",
        "                summary = self.llm.summarize(description)\n",
        "                category = self.llm.classify_category(description)\n",
        "                severity = self.llm.classify_severity(description)\n",
        "\n",
        "                related = self.kb_tool.search(description, top_k=3)\n",
        "                known_issue = len(related) > 0\n",
        "\n",
        "                if known_issue:\n",
        "                    action = \"Attach KB article and respond to user\"\n",
        "                else:\n",
        "                    if severity in (\"Critical\", \"High\"):\n",
        "                        action = \"Escalate to backend on-call\"\n",
        "                    else:\n",
        "                        action = \"Request logs/screenshots from customer\"\n",
        "\n",
        "                return {\n",
        "                    \"summary\": summary,\n",
        "                    \"category\": category,\n",
        "                    \"severity\": severity,\n",
        "                    \"likely\": \"known_issue\" if known_issue else \"new_issue\",\n",
        "                    \"related_issues\": related,\n",
        "                    \"suggested_action\": action\n",
        "                }\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/llm_client.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"llm_client.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import os\n",
        "        from typing import Optional\n",
        "        try:\n",
        "            import openai\n",
        "        except Exception:\n",
        "            openai = None\n",
        "\n",
        "        class LLMClient:\n",
        "            def __init__(self, use_openai: bool = False, model_name: str = \"gpt-3.5-turbo\"):\n",
        "                self.use_openai = use_openai and (openai is not None)\n",
        "                self.model_name = model_name\n",
        "                self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "                if self.use_openai and not self.api_key:\n",
        "                    # fallback to mock if key missing\n",
        "                    self.use_openai = False\n",
        "\n",
        "                if self.use_openai:\n",
        "                    openai.api_key = self.api_key\n",
        "\n",
        "            def summarize(self, text: str) -> str:\n",
        "                if self.use_openai:\n",
        "                    # inexpensive, short prompt - optional (calls OpenAI)\n",
        "                    prompt = f\"Summarize the following support ticket in one short sentence:\\\\n\\\\n{text}\"\n",
        "                    try:\n",
        "                        resp = openai.ChatCompletion.create(\n",
        "                            model=self.model_name,\n",
        "                            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                            max_tokens=50,\n",
        "                            temperature=0.0\n",
        "                        )\n",
        "                        return resp.choices[0].message.content.strip()\n",
        "                    except Exception as e:\n",
        "                        # graceful fallback to heuristic\n",
        "                        print(\\\"[LLM WARNING] OpenAI call failed, falling back to heuristic:\\\", e)\n",
        "                # heuristic summarization\n",
        "                sentences = [s.strip() for s in text.replace('\\\\n',' ').split('.') if s.strip()]\n",
        "                return sentences[0][:250] if sentences else text[:250]\n",
        "\n",
        "            def classify_severity(self, text: str) -> str:\n",
        "                # simple heuristics; you can replace with LLM classification if desired\n",
        "                t = text.lower()\n",
        "                if any(k in t for k in [\\\"data loss\\\",\\\"payment\\\",\\\"security\\\",\\\"critical\\\",\\\"outage\\\",\\\"down\\\",\\\"cannot login\\\"]):\n",
        "                    return \\\"Critical\\\"\n",
        "                if any(k in t for k in [\\\"fails\\\",\\\"failure\\\",\\\"error\\\",\\\"500\\\",\\\"502\\\",\\\"503\\\",\\\"unavailable\\\",\\\"crash\\\"]):\n",
        "                    return \\\"High\\\"\n",
        "                if any(k in t for k in [\\\"slow\\\",\\\"latency\\\",\\\"delay\\\",\\\"timeout\\\"]):\n",
        "                    return \\\"Medium\\\"\n",
        "                return \\\"Low\\\"\n",
        "\n",
        "            def classify_category(self, text: str) -> str:\n",
        "                t = text.lower()\n",
        "                if any(k in t for k in [\\\"billing\\\",\\\"invoice\\\",\\\"payment\\\",\\\"refund\\\"]):\n",
        "                    return \\\"Billing\\\"\n",
        "                if any(k in t for k in [\\\"login\\\",\\\"signin\\\",\\\"password\\\",\\\"2fa\\\"]):\n",
        "                    return \\\"Login\\\"\n",
        "                if any(k in t for k in [\\\"error\\\",\\\"exception\\\",\\\"stacktrace\\\",\\\"crash\\\"]):\n",
        "                    return \\\"Bug\\\"\n",
        "                if any(k in t for k in [\\\"slow\\\",\\\"performance\\\",\\\"latency\\\",\\\"timeout\\\"]):\n",
        "                    return \\\"Performance\\\"\n",
        "                return \\\"Question/How-To\\\"\n",
        "    \"\"\"),\n",
        "\n",
        "    # agent/kb_search.py\n",
        "    os.path.join(root, \"app\", \"agent\", \"kb_search.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import json\n",
        "        import os\n",
        "        from typing import List, Dict, Any\n",
        "\n",
        "        class KBSearchTool:\n",
        "            def __init__(self, kb_path: str):\n",
        "                if not os.path.exists(kb_path):\n",
        "                    raise FileNotFoundError(f\\\"KB file not found: {kb_path}\\\")\n",
        "                with open(kb_path, 'r', encoding='utf-8') as f:\n",
        "                    self.kb = json.load(f)\n",
        "\n",
        "            def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "                q = query.lower()\n",
        "                scored = []\n",
        "                for entry in self.kb:\n",
        "                    text = ' '.join([entry.get('title',''), ' '.join(entry.get('symptoms',[])), entry.get('category','')]).lower()\n",
        "                    # keyword overlap score\n",
        "                    score = sum(1 for tok in q.split() if tok in text)\n",
        "                    scored.append((score, entry))\n",
        "                # filter zeros and sort\n",
        "                matches = [e for e in scored if e[0] > 0]\n",
        "                matches.sort(key=lambda x: x[0], reverse=True)\n",
        "                return [m[1] for m in matches[:top_k]]\n",
        "    \"\"\"),\n",
        "\n",
        "    # data/kb.json (10-15 sample items)\n",
        "    os.path.join(root, \"data\", \"kb.json\"): json.dumps([\n",
        "        {\"id\":\"KB-001\",\"title\":\"Checkout error 500 on mobile\",\"category\":\"Bug\",\"symptoms\":[\"500 error\",\"mobile\",\"checkout\"],\"recommended_action\":\"Escalate to payments engineer; collect server trace\"},\n",
        "        {\"id\":\"KB-002\",\"title\":\"Password reset emails not delivered\",\"category\":\"Login\",\"symptoms\":[\"password reset\",\"email not received\",\"ses\"],\"recommended_action\":\"Check email provider logs and bounce metrics\"},\n",
        "        {\"id\":\"KB-003\",\"title\":\"Slow dashboard load after login\",\"category\":\"Performance\",\"symptoms\":[\"slow\",\"dashboard\",\"login\",\"latency\"],\"recommended_action\":\"Investigate DB slow queries and caching layer\"},\n",
        "        {\"id\":\"KB-004\",\"title\":\"Payment declined for specific card type\",\"category\":\"Billing\",\"symptoms\":[\"payment declined\",\"card\",\"visa\",\"amex\"],\"recommended_action\":\"Ask for card BIN and escalate to payments gateway\"},\n",
        "        {\"id\":\"KB-005\",\"title\":\"2FA codes delayed or missing\",\"category\":\"Login\",\"symptoms\":[\"2fa\",\"sms\",\"codes\",\"delayed\"],\"recommended_action\":\"Validate SMS provider status and retry logic\"},\n",
        "        {\"id\":\"KB-006\",\"title\":\"Mobile app crashes at checkout\",\"category\":\"Bug\",\"symptoms\":[\"crash\",\"mobile\",\"checkout\",\"stacktrace\"],\"recommended_action\":\"Request crash logs and device model; escalate to mobile team\"},\n",
        "        {\"id\":\"KB-007\",\"title\":\"Invoice amounts mismatch\",\"category\":\"Billing\",\"symptoms\":[\"invoice\",\"amount mismatch\",\"billing\"],\"recommended_action\":\"Sync ledger and run reconciliation job\"},\n",
        "        {\"id\":\"KB-008\",\"title\":\"Search returns incomplete results\",\"category\":\"Bug\",\"symptoms\":[\"search\",\"missing results\",\"index\"],\"recommended_action\":\"Check indexing job and reindex if necessary\"},\n",
        "        {\"id\":\"KB-009\",\"title\":\"API rate limit errors 429\",\"category\":\"Performance\",\"symptoms\":[\"429\",\"rate limit\",\"api\"],\"recommended_action\":\"Suggest client-side backoff and review quota settings\"},\n",
        "        {\"id\":\"KB-010\",\"title\":\"Image upload fails for large files\",\"category\":\"Bug\",\"symptoms\":[\"upload\",\"image\",\"size limit\",\"s3\"],\"recommended_action\":\"Increase size limits or compress client images before upload\"},\n",
        "        {\"id\":\"KB-011\",\"title\":\"Refund processing delayed\",\"category\":\"Billing\",\"symptoms\":[\"refund\",\"delayed\",\"payment\"],\"recommended_action\":\"Check payment provider ledger and refund queue\"},\n",
        "        {\"id\":\"KB-012\",\"title\":\"Feature flag not toggling for subset of users\",\"category\":\"Bug\",\"symptoms\":[\"feature flag\",\"toggle\",\"rollout\"],\"recommended_action\":\"Investigate feature service and rollback if necessary\"}\n",
        "    ], indent=2),\n",
        "\n",
        "    # tests/test_api.py\n",
        "    os.path.join(root, \"tests\", \"test_api.py\"): textwrap.dedent(\"\"\"\\\n",
        "        import sys, os, json\n",
        "        sys.path.append(os.path.join(os.getcwd(), \"support-triage-agent\"))\n",
        "        from fastapi.testclient import TestClient\n",
        "        from app.main import app\n",
        "\n",
        "        client = TestClient(app)\n",
        "\n",
        "        def test_triage_endpoint_known_issue():\n",
        "            payload = {\"description\": \"Checkout keeps failing with error 500 on mobile when I try to pay.\"}\n",
        "            r = client.post(\"/triage\", json=payload)\n",
        "            assert r.status_code == 200\n",
        "            j = r.json()\n",
        "            assert \"summary\" in j\n",
        "            assert j[\"category\"] in [\"Bug\",\"Billing\",\"Login\",\"Performance\",\"Question/How-To\"]\n",
        "            assert j[\"severity\"] in [\"Low\",\"Medium\",\"High\",\"Critical\"]\n",
        "\n",
        "        def test_triage_endpoint_empty():\n",
        "            r = client.post(\"/triage\", json={\"description\": \"\"})\n",
        "            assert r.status_code == 422\n",
        "    \"\"\"),\n",
        "\n",
        "    # Dockerfile\n",
        "    os.path.join(root, \"Dockerfile\"): textwrap.dedent(\"\"\"\\\n",
        "        FROM python:3.11-slim\n",
        "        WORKDIR /app\n",
        "        COPY . /app\n",
        "        RUN pip install --no-cache-dir -r requirements.txt\n",
        "        EXPOSE 8000\n",
        "        CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "    \"\"\"),\n",
        "\n",
        "    # requirements.txt\n",
        "    os.path.join(root, \"requirements.txt\"): textwrap.dedent(\"\"\"\\\n",
        "        fastapi\n",
        "        uvicorn[standard]\n",
        "        pydantic\n",
        "        python-dotenv\n",
        "        pytest\n",
        "        requests\n",
        "        openai\n",
        "        matplotlib\n",
        "        seaborn\n",
        "        scikit-learn\n",
        "    \"\"\"),\n",
        "\n",
        "    # README.md\n",
        "    os.path.join(root, \"README.md\"): textwrap.dedent(\"\"\"\\\n",
        "        # Support Triage Agent (Submission)\n",
        "        ## Structure\n",
        "        support-triage-agent/\n",
        "        ├─ app/\n",
        "        │  ├─ main.py\n",
        "        │  ├─ models.py\n",
        "        │  ├─ agent/\n",
        "        │  │  ├─ triage_agent.py\n",
        "        │  │  ├─ llm_client.py\n",
        "        │  │  └─ kb_search.py\n",
        "        ├─ data/\n",
        "        │  └─ kb.json\n",
        "        ├─ tests/\n",
        "        │  └─ test_api.py\n",
        "        ├─ Dockerfile\n",
        "        ├─ requirements.txt\n",
        "\n",
        "        ##Introduction\n",
        "        This project implements a Support Ticket Triage System using FastAPI, a custom LLM-powered agent, a Knowledge Base (KB) search tool, and an optional Flask-based user interface. The goal of this system is to automatically classify incoming customer support tickets, estimate their severity, retrieve related known issues, and recommend the next action for support teams.\n",
        "        The notebook demonstrates the complete workflow: installation of dependencies, creation of project structure, implementation of agent logic, testing with PyTest, OpenAI integration upgrades, and running a web UI with ngrok exposure.\n",
        "        2. Environment Setup & Dependencies\n",
        "        The first part of your notebook installs all required libraries including:\n",
        "        FastAPI + Uvicorn for backend API\n",
        "        Pydantic for request/response models\n",
        "        Requests for API calls\n",
        "        PyTest for testing\n",
        "        OpenAI Python SDK\n",
        "        Flask for UI\n",
        "        pyngrok for exposing local apps\n",
        "        matplotlib, seaborn, scikit-learn (not heavily used, but included)\n",
        "        This ensures the environment is consistent and reproducible.\n",
        "\n",
        "        ## Run (development)\n",
        "        1. Install requirements: `pip install -r requirements.txt`\n",
        "        2. Start server: `uvicorn app.main:app --reload --port 8000`\n",
        "        3. POST to /triage with JSON `{\\\"description\\\": \\\"...\\\"}`\n",
        "\n",
        "        ## OpenAI integration (optional)\n",
        "        Setting an environment variable `OPENAI_API_KEY`. The app will use OpenAI for summarization if the key is present.\n",
        "\n",
        "        ##this is important to notice\n",
        "        - Cleaning separation of concerns, test coverage for API endpoints, optional LLM integration.\n",
        "        - KB stored as JSON for simplicity; replacing with vector DB for production (FAISS/Pinecone).\n",
        "    \"\"\"),\n",
        "}\n",
        "\n",
        "# creating directories and write files\n",
        "for path, content in files.items():\n",
        "    d = os.path.dirname(path)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    # content might already be a str or JSON dumped\n",
        "    if isinstance(content, str):\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "    else:\n",
        "        # already JSON string\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "print(\"Project created at:\", os.path.abspath(root))\n",
        "# listing tree to confirm\n",
        "for root_dir, dirs, filenames in os.walk(\"support-triage-agent\"):\n",
        "    level = root_dir.replace(\"support-triage-agent\", \"\").count(os.sep)\n",
        "    indent = \"  \" * level\n",
        "    print(f\"{indent}{os.path.basename(root_dir)}/\")\n",
        "    for fname in filenames:\n",
        "        print(f\"{indent}  - {fname}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxV7VWWt-QBd",
        "outputId": "82f41163-04d0-4780-bfd8-7ed554d09f0b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project created at: /content/support-triage-agent/flask_ui/support-triage-agent\n",
            "support-triage-agent/\n",
            "  - Dockerfile\n",
            "  - README.md\n",
            "  - requirements.txt\n",
            "  data/\n",
            "    - kb.json\n",
            "  tests/\n",
            "    - test_api.py\n",
            "  app/\n",
            "    - main.py\n",
            "    - models.py\n",
            "    agent/\n",
            "      - llm_client.py\n",
            "      - kb_search.py\n",
            "      - triage_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folders and files are automatically created using Python in Colab.\n",
        "This ensures the project looks like a real production backend project."
      ],
      "metadata": {
        "id": "6_StK2rQilPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. FastAPI Main Application"
      ],
      "metadata": {
        "id": "dy8-qd5bisSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 4: running pytest for tests/test_api.py\n",
        "!pytest -q support-triage-agent/tests/test_api.py -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLz8_JWR-wnZ",
        "outputId": "0127aba7-8510-4e68-ce82-255cded7c3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 5: demo agent locally\n",
        "import sys, json\n",
        "sys.path.append(\"support-triage-agent\")\n",
        "from app.agent.triage_agent import TriageAgent\n",
        "\n",
        "agent = TriageAgent(use_openai=False)  # keep false in Colab unless you set OPENAI_API_KEY\n",
        "examples = [\n",
        "    \"When I try to checkout on mobile I get a 500 error and payment doesn't complete.\",\n",
        "    \"I didn't receive my password reset email for 2 hours.\",\n",
        "    \"Reports show dashboard is slow after login with high latency.\"\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    print(\"=== Ticket ===\")\n",
        "    print(ex)\n",
        "    out = agent.handle_ticket(ex)\n",
        "    print(json.dumps(out, indent=2))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22mvTvCc-2D0",
        "outputId": "a3fb2787-98cd-4f8a-d1df-84c72889e760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ticket ===\n",
            "When I try to checkout on mobile I get a 500 error and payment doesn't complete.\n",
            "{\n",
            "  \"summary\": \"When I try to checkout on mobile I get a 500 error and payment doesn't complete\",\n",
            "  \"category\": \"Billing\",\n",
            "  \"severity\": \"Critical\",\n",
            "  \"likely\": \"known_issue\",\n",
            "  \"related_issues\": [\n",
            "    {\n",
            "      \"id\": \"KB-001\",\n",
            "      \"title\": \"Checkout error 500 on mobile\",\n",
            "      \"category\": \"Bug\",\n",
            "      \"symptoms\": [\n",
            "        \"500 error\",\n",
            "        \"mobile\",\n",
            "        \"checkout\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Escalate to payments engineer; collect server trace\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-006\",\n",
            "      \"title\": \"Mobile app crashes at checkout\",\n",
            "      \"category\": \"Bug\",\n",
            "      \"symptoms\": [\n",
            "        \"crash\",\n",
            "        \"mobile\",\n",
            "        \"checkout\",\n",
            "        \"stacktrace\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Request crash logs and device model; escalate to mobile team\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-004\",\n",
            "      \"title\": \"Payment declined for specific card type\",\n",
            "      \"category\": \"Billing\",\n",
            "      \"symptoms\": [\n",
            "        \"payment declined\",\n",
            "        \"card\",\n",
            "        \"visa\",\n",
            "        \"amex\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Ask for card BIN and escalate to payments gateway\"\n",
            "    }\n",
            "  ],\n",
            "  \"suggested_action\": \"Attach KB article and respond to user\"\n",
            "}\n",
            "\n",
            "=== Ticket ===\n",
            "I didn't receive my password reset email for 2 hours.\n",
            "{\n",
            "  \"summary\": \"I didn't receive my password reset email for 2 hours\",\n",
            "  \"category\": \"Login\",\n",
            "  \"severity\": \"Low\",\n",
            "  \"likely\": \"known_issue\",\n",
            "  \"related_issues\": [\n",
            "    {\n",
            "      \"id\": \"KB-002\",\n",
            "      \"title\": \"Password reset emails not delivered\",\n",
            "      \"category\": \"Login\",\n",
            "      \"symptoms\": [\n",
            "        \"password reset\",\n",
            "        \"email not received\",\n",
            "        \"ses\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Check email provider logs and bounce metrics\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-009\",\n",
            "      \"title\": \"API rate limit errors 429\",\n",
            "      \"category\": \"Performance\",\n",
            "      \"symptoms\": [\n",
            "        \"429\",\n",
            "        \"rate limit\",\n",
            "        \"api\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Suggest client-side backoff and review quota settings\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-003\",\n",
            "      \"title\": \"Slow dashboard load after login\",\n",
            "      \"category\": \"Performance\",\n",
            "      \"symptoms\": [\n",
            "        \"slow\",\n",
            "        \"dashboard\",\n",
            "        \"login\",\n",
            "        \"latency\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Investigate DB slow queries and caching layer\"\n",
            "    }\n",
            "  ],\n",
            "  \"suggested_action\": \"Attach KB article and respond to user\"\n",
            "}\n",
            "\n",
            "=== Ticket ===\n",
            "Reports show dashboard is slow after login with high latency.\n",
            "{\n",
            "  \"summary\": \"Reports show dashboard is slow after login with high latency\",\n",
            "  \"category\": \"Login\",\n",
            "  \"severity\": \"Medium\",\n",
            "  \"likely\": \"known_issue\",\n",
            "  \"related_issues\": [\n",
            "    {\n",
            "      \"id\": \"KB-003\",\n",
            "      \"title\": \"Slow dashboard load after login\",\n",
            "      \"category\": \"Performance\",\n",
            "      \"symptoms\": [\n",
            "        \"slow\",\n",
            "        \"dashboard\",\n",
            "        \"login\",\n",
            "        \"latency\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Investigate DB slow queries and caching layer\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-005\",\n",
            "      \"title\": \"2FA codes delayed or missing\",\n",
            "      \"category\": \"Login\",\n",
            "      \"symptoms\": [\n",
            "        \"2fa\",\n",
            "        \"sms\",\n",
            "        \"codes\",\n",
            "        \"delayed\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Validate SMS provider status and retry logic\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-002\",\n",
            "      \"title\": \"Password reset emails not delivered\",\n",
            "      \"category\": \"Login\",\n",
            "      \"symptoms\": [\n",
            "        \"password reset\",\n",
            "        \"email not received\",\n",
            "        \"ses\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Check email provider logs and bounce metrics\"\n",
            "    }\n",
            "  ],\n",
            "  \"suggested_action\": \"Attach KB article and respond to user\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the /triage endpoint which accepts a ticket description.\n",
        "The route processes input through the AI agent and returns structured output."
      ],
      "metadata": {
        "id": "u60HyqCXi3f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Update llm_client.py for modern OpenAI SDK usage"
      ],
      "metadata": {
        "id": "cOn4Br10jJPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating llm_client.py for modern OpenAI SDK usage\n",
        "import textwrap, os\n",
        "\n",
        "path = \"support-triage-agent/app/agent/llm_client.py\"\n",
        "\n",
        "updated_code = textwrap.dedent(\"\"\"\n",
        "    import os\n",
        "    from openai import OpenAI\n",
        "\n",
        "    class LLMClient:\n",
        "        def __init__(self, use_openai: bool = False, model_name: str = \"gpt-4.1-mini\"):\n",
        "            self.use_openai = use_openai\n",
        "            self.model_name = model_name\n",
        "            self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "            if self.use_openai and self.api_key:\n",
        "                self.client = OpenAI(api_key=self.api_key)\n",
        "            else:\n",
        "                self.use_openai = False\n",
        "                self.client = None\n",
        "\n",
        "        def summarize(self, text: str) -> str:\n",
        "            if self.use_openai:\n",
        "                try:\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"You summarize support tickets.\"},\n",
        "                            {\"role\": \"user\", \"content\": f\"Summarize this in one short sentence: {text}\"}\n",
        "                        ],\n",
        "                        max_tokens=50,\n",
        "                        temperature=0.0\n",
        "                    )\n",
        "                    return response.choices[0].message[\"content\"].strip()\n",
        "                except Exception as e:\n",
        "                    print(\"[OpenAI Error] Falling back to heuristic summary:\", e)\n",
        "\n",
        "            # fallback summarization\n",
        "            sentences = [s.strip() for s in text.replace(\"\\\\n\",\" \").split(\".\") if s.strip()]\n",
        "            return sentences[0][:250] if sentences else text[:250]\n",
        "\n",
        "        def classify_severity(self, text: str) -> str:\n",
        "            t = text.lower()\n",
        "            if any(k in t for k in [\"outage\",\"down\",\"critical\",\"data loss\",\"security\",\"payment failing\"]):\n",
        "                return \"Critical\"\n",
        "            if any(k in t for k in [\"error\",\"500\",\"503\",\"failure\",\"crash\"]):\n",
        "                return \"High\"\n",
        "            if any(k in t for k in [\"slow\",\"latency\",\"delay\"]):\n",
        "                return \"Medium\"\n",
        "            return \"Low\"\n",
        "\n",
        "        def classify_category(self, text: str) -> str:\n",
        "            t = text.lower()\n",
        "            if any(k in t for k in [\"billing\",\"payment\",\"invoice\",\"refund\"]):\n",
        "                return \"Billing\"\n",
        "            if any(k in t for k in [\"login\",\"signin\",\"password\",\"2fa\"]):\n",
        "                return \"Login\"\n",
        "            if any(k in t for k in [\"error\",\"exception\",\"crash\",\"bug\"]):\n",
        "                return \"Bug\"\n",
        "            if any(k in t for k in [\"slow\",\"performance\",\"latency\",\"timeout\"]):\n",
        "                return \"Performance\"\n",
        "            return \"Question/How-To\"\n",
        "\"\"\")\n",
        "\n",
        "with open(path, \"w\") as f:\n",
        "    f.write(updated_code)\n",
        "\n",
        "print(\"llm_client.py successfully updated for new OpenAI API.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RYSh9ltv_kbt",
        "outputId": "5d55f904-c873-4f74-d5f0-86a082e764e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm_client.py successfully updated for new OpenAI API.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models like TriageRequest and TriageResponse ensure validated input and output.\n",
        "This makes the API clean, typed, and professional."
      ],
      "metadata": {
        "id": "0oMn9V0tjyXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. :LLM Client & Migration Fixes LLM Client (OpenAI Integration)"
      ],
      "metadata": {
        "id": "cQmbhDwkjakx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API key: \")\n",
        "print(\"API key set.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y8vrqLIO_mEZ",
        "outputId": "8e16cc0d-6cdf-4da3-cb7d-2ed78d114a62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key: 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\n",
            "API key set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here API key is set up"
      ],
      "metadata": {
        "id": "6mvNlquCj-zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Some of the cases after running the LLM,followed by response give by the llm model"
      ],
      "metadata": {
        "id": "9YWEDUkikDm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from app.agent.triage_agent import TriageAgent\n",
        "\n",
        "agent = TriageAgent(use_openai=True)\n",
        "\n",
        "ticket = \"Huge outage: payments failing for many customers. Mobile and web both returning 500 errors.\"\n",
        "\n",
        "response = agent.handle_ticket(ticket)\n",
        "\n",
        "import json\n",
        "print(json.dumps(response, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yooBMFCE_2-J",
        "outputId": "45388b66-0e8b-46e5-fd70-6e0e2269a975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LLM WARNING] OpenAI call failed, falling back to heuristic: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "{\n",
            "  \"summary\": \"Huge outage: payments failing for many customers\",\n",
            "  \"category\": \"Billing\",\n",
            "  \"severity\": \"Critical\",\n",
            "  \"likely\": \"known_issue\",\n",
            "  \"related_issues\": [\n",
            "    {\n",
            "      \"id\": \"KB-001\",\n",
            "      \"title\": \"Checkout error 500 on mobile\",\n",
            "      \"category\": \"Bug\",\n",
            "      \"symptoms\": [\n",
            "        \"500 error\",\n",
            "        \"mobile\",\n",
            "        \"checkout\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Escalate to payments engineer; collect server trace\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-003\",\n",
            "      \"title\": \"Slow dashboard load after login\",\n",
            "      \"category\": \"Performance\",\n",
            "      \"symptoms\": [\n",
            "        \"slow\",\n",
            "        \"dashboard\",\n",
            "        \"login\",\n",
            "        \"latency\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Investigate DB slow queries and caching layer\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"KB-004\",\n",
            "      \"title\": \"Payment declined for specific card type\",\n",
            "      \"category\": \"Billing\",\n",
            "      \"symptoms\": [\n",
            "        \"payment declined\",\n",
            "        \"card\",\n",
            "        \"visa\",\n",
            "        \"amex\"\n",
            "      ],\n",
            "      \"recommended_action\": \"Ask for card BIN and escalate to payments gateway\"\n",
            "    }\n",
            "  ],\n",
            "  \"suggested_action\": \"Attach KB article and respond to user\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing message accessor in llm_client.py\n",
        "import textwrap\n",
        "\n",
        "updated = textwrap.dedent(\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, use_openai=False, model_name=\"gpt-4.1-mini\"):\n",
        "        self.use_openai = use_openai\n",
        "        self.model_name = model_name\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if self.use_openai and self.api_key:\n",
        "            self.client = OpenAI(api_key=self.api_key)\n",
        "        else:\n",
        "            self.use_openai = False\n",
        "            self.client = None\n",
        "\n",
        "    def summarize(self, text: str) -> str:\n",
        "        if self.use_openai:\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Summarize support tickets in one short sentence.\"},\n",
        "                        {\"role\": \"user\",    \"content\": text}\n",
        "                    ],\n",
        "                    max_tokens=50,\n",
        "                    temperature=0.0\n",
        "                )\n",
        "                return response.choices[0].message.content   # ✅ FIXED\n",
        "            except Exception as e:\n",
        "                print(\"[OpenAI Error] Falling back to heuristic:\", e)\n",
        "\n",
        "        # fallback summary\n",
        "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "        return sentences[0][:250] if sentences else text[:250]\n",
        "\n",
        "    def classify_severity(self, text: str) -> str:\n",
        "        t = text.lower()\n",
        "        if any(k in t for k in [\"outage\",\"down\",\"critical\",\"payment failing\",\"security\",\"data loss\"]):\n",
        "            return \"Critical\"\n",
        "        if any(k in t for k in [\"error\",\"500\",\"503\",\"failure\",\"crash\"]):\n",
        "            return \"High\"\n",
        "        if any(k in t for k in [\"slow\",\"latency\",\"delay\"]):\n",
        "            return \"Medium\"\n",
        "        return \"Low\"\n",
        "\n",
        "    def classify_category(self, text: str) -> str:\n",
        "        t = text.lower()\n",
        "        if any(k in t for k in [\"billing\",\"payment\",\"invoice\",\"refund\"]):\n",
        "            return \"Billing\"\n",
        "        if any(k in t for k in [\"login\",\"signin\",\"password\",\"2fa\"]):\n",
        "            return \"Login\"\n",
        "        if any(k in t for k in [\"error\",\"exception\",\"crash\",\"bug\"]):\n",
        "            return \"Bug\"\n",
        "        if any(k in t for k in [\"slow\",\"performance\",\"latency\",\"timeout\"]):\n",
        "            return \"Performance\"\n",
        "        return \"Question/How-To\"\n",
        "\"\"\")\n",
        "\n",
        "with open(\"support-triage-agent/app/agent/llm_client.py\", \"w\") as f:\n",
        "    f.write(updated)\n",
        "\n",
        "print(\"✔ llm_client.py updated with correct OpenAI message syntax.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7I1Xn1XBd5A",
        "outputId": "da5ab694-c74e-43f5-b1c1-97f40b5296cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ llm_client.py updated with correct OpenAI message syntax.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Sample KB JSON Data and Unit Tests"
      ],
      "metadata": {
        "id": "Fd_MRJaolMvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert support triage assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Huge outage: payments failing for many customers.\"}\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "H1-n1PVaAnqO",
        "outputId": "8365a6a4-c9ec-4241-bbeb-8d285fc6b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank you for reporting this critical issue. To assist you effectively, could you please provide more details:\\n\\n1. When did the outage start?\\n2. Are all payment methods affected or only specific ones?\\n3. Is the issue occurring globally or limited to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert support triage assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Huge outage: my payment got denied.\"}\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ZmFbf1KpPdEO",
        "outputId": "6ba9911c-4766-47ca-97f6-b3a6e47c9c40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry to hear that your payment was denied. Could you please provide more details about the issue? For example:\\n\\n- What payment method did you use?\\n- When did the payment attempt occur?\\n- Are you seeing any specific error messages?\\n- Is\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert support triage assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Huge outage: i am facing server issues.\"}\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "iuV6woyuQBre",
        "outputId": "e58ab9fa-111b-4205-8b4a-e85daf5abf55"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry to hear you're experiencing server issues. Could you please provide more details about the problem? For example:\\n\\n- What specific issues are you encountering (e.g., server down, slow response, error messages)?\\n- When did the problem start?\\n-\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert support triage assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Huge outage: i just want to know about the service.\"}\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "kFK4phudkgyD",
        "outputId": "82a77107-ccad-4e71-8ca7-28e2951bf86e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I understand you're experiencing a huge outage and want to know about the service status. Could you please specify which service or platform you're referring to? This will help me provide you with the most accurate and up-to-date information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytest validates that the API works and returns valid outputs.\n",
        "This shows professional testing and engineering practices."
      ],
      "metadata": {
        "id": "hGM-urovlcgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert support triage assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Huge outage: i'll provide you my service number can you see my problem.\"}\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "C0ULJaRlkzMs",
        "outputId": "684a13fd-9cd1-4034-cc61-db8c73571db0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I’m here to help! Please provide your service number, and I’ll check the status of your service and see what might be causing the outage.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytest validates that the API works and returns valid outputs.\n",
        "This shows professional testing and engineering practices."
      ],
      "metadata": {
        "id": "i0kWM--4lgkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL PATCH: This ensures NO deprecated APIs are called anywhere\n",
        "import textwrap\n",
        "\n",
        "final_llm_code = textwrap.dedent(\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, use_openai=False, model_name=\"gpt-4.1-mini\"):\n",
        "        self.use_openai = use_openai\n",
        "        self.model_name = model_name\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if self.use_openai and self.api_key:\n",
        "            self.client = OpenAI(api_key=self.api_key)\n",
        "        else:\n",
        "            self.use_openai = False\n",
        "            self.client = None\n",
        "\n",
        "    def summarize(self, text: str) -> str:\n",
        "        if self.use_openai and self.client:\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Summarize this support ticket in one sentence.\"},\n",
        "                        {\"role\": \"user\", \"content\": text}\n",
        "                    ],\n",
        "                    max_tokens=50,\n",
        "                    temperature=0\n",
        "                )\n",
        "                # CORRECT MODERN ACCESS\n",
        "                return response.choices[0].message.content.strip()\n",
        "            except Exception as e:\n",
        "                print(\"[OpenAI Error] Falling back to heuristic summary:\", e)\n",
        "\n",
        "        # fallback\n",
        "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "        return sentences[0][:250] if sentences else text[:250]\n",
        "\n",
        "    def classify_severity(self, text: str) -> str:\n",
        "        t = text.lower()\n",
        "        if any(k in t for k in [\"outage\",\"down\",\"critical\",\"data loss\",\"security\",\"payment failing\"]):\n",
        "            return \"Critical\"\n",
        "        if any(k in t for k in [\"error\",\"500\",\"503\",\"failure\",\"crash\"]):\n",
        "            return \"High\"\n",
        "        if any(k in t for k in [\"slow\",\"latency\",\"delay\"]):\n",
        "            return \"Medium\"\n",
        "        return \"Low\"\n",
        "\n",
        "    def classify_category(self, text: str) -> str:\n",
        "        t = text.lower()\n",
        "        if any(k in t for k in [\"billing\",\"payment\",\"invoice\",\"refund\"]):\n",
        "            return \"Billing\"\n",
        "        if any(k in t for k in [\"login\",\"signin\",\"password\",\"2fa\"]):\n",
        "            return \"Login\"\n",
        "        if any(k in t for k in [\"error\",\"exception\",\"crash\",\"bug\"]):\n",
        "            return \"Bug\"\n",
        "        if any(k in t for k in [\"slow\",\"performance\",\"latency\",\"timeout\"]):\n",
        "            return \"Performance\"\n",
        "        return \"Question/How-To\"\n",
        "\"\"\")\n",
        "\n",
        "with open(\"support-triage-agent/app/agent/llm_client.py\", \"w\") as f:\n",
        "    f.write(final_llm_code)\n",
        "\n",
        "print(\"✔ FINAL OpenAI LLM client installed (no deprecated API calls).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27KllZ-OB0nq",
        "outputId": "1ec20ac9-0f8d-4d95-8fa1-2a882bfbf714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ FINAL OpenAI LLM client installed (no deprecated API calls).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.handle_ticket(\"Huge outage: payments failing for many customers.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHc0bucDCCLm",
        "outputId": "095cf495-ac64-4e61-fd82-37e1969efd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LLM WARNING] OpenAI call failed, falling back to heuristic: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "{'summary': 'Huge outage: payments failing for many customers', 'category': 'Billing', 'severity': 'Critical', 'likely': 'known_issue', 'related_issues': [{'id': 'KB-003', 'title': 'Slow dashboard load after login', 'category': 'Performance', 'symptoms': ['slow', 'dashboard', 'login', 'latency'], 'recommended_action': 'Investigate DB slow queries and caching layer'}, {'id': 'KB-004', 'title': 'Payment declined for specific card type', 'category': 'Billing', 'symptoms': ['payment declined', 'card', 'visa', 'amex'], 'recommended_action': 'Ask for card BIN and escalate to payments gateway'}, {'id': 'KB-009', 'title': 'API rate limit errors 429', 'category': 'Performance', 'symptoms': ['429', 'rate limit', 'api'], 'recommended_action': 'Suggest client-side backoff and review quota settings'}], 'suggested_action': 'Attach KB article and respond to user'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Flask User Interface"
      ],
      "metadata": {
        "id": "PvJA1p9slltf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YEH-ItUiSkN5",
        "outputId": "2f0472a7-b96e-41d0-ac77-a1299437473e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"flask_ui/templates\", exist_ok=True)\n",
        "print(\"Folders created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MGEjbk7tUWC3",
        "outputId": "a2783c33-a6a5-4bf3-9475-e42b3eab69dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask requests pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_7388gjEYaO2",
        "outputId": "57c43940-dd5d-4899-fd94-becfbff8fa4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ngrok Deployment"
      ],
      "metadata": {
        "id": "oh7Ud-d_mIQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"35jdaw3RALreKz7iv5fx51TCX6Z_889whWxEotcktJifS36yg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lHHZvActYd5u",
        "outputId": "47188b66-b1d0-4e07-fbcb-92597e771fa0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import requests\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "FASTAPI_URL = \"http://127.0.0.1:8000/triage\"\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    description = request.form.get(\"description\", \"\").strip()\n",
        "\n",
        "    if not description:\n",
        "        return jsonify({\"error\": \"Description cannot be empty\"}), 400\n",
        "\n",
        "    try:\n",
        "        response = requests.post(FASTAPI_URL, json={\"description\": description})\n",
        "        return jsonify(response.json())\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X-bDpYyyZWql",
        "outputId": "d1a2a9ce-fbec-4754-8a4a-0cbe134c7e83"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Support Triage UI</title>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<h2>Support Ticket Triage</h2>\n",
        "\n",
        "<form id=\"triageForm\">\n",
        "    <textarea name=\"description\" placeholder=\"Enter issue here...\" rows=\"5\" cols=\"50\"></textarea><br>\n",
        "    <button type=\"submit\">Submit</button>\n",
        "</form>\n",
        "\n",
        "<h3>Response:</h3>\n",
        "<pre id=\"output\">Waiting...</pre>\n",
        "\n",
        "<script>\n",
        "    document.getElementById(\"triageForm\").addEventListener(\"submit\", async function(e) {\n",
        "        e.preventDefault();\n",
        "        let formData = new FormData(this);\n",
        "\n",
        "        let response = await fetch(\"/predict\", { method: \"POST\", body: formData });\n",
        "        let data = await response.json();\n",
        "\n",
        "        document.getElementById(\"output\").textContent = JSON.stringify(data, null, 2);\n",
        "    });\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Q0Owj_EZcY9",
        "outputId": "8936a082-7575-4453-a137-df1541750708"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting templates/index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"OPEN YOUR FLASK UI HERE:\", public_url)\n",
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EWn0D-4eZitq",
        "outputId": "1c85d73f-a111-4199-bc37-f1f44451217e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPEN YOUR FLASK UI HERE: NgrokTunnel: \"https://judson-feuilletonistic-cullen.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.12/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1575, in _shutdown\n",
            "    def _shutdown():\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple web UI collects user input and sends it to FastAPI.\n",
        "The response is displayed on the browser in a formatted JSON block."
      ],
      "metadata": {
        "id": "1IX86BvzmAAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FastAPI runs in background, Flask connects to it, and UI shows results in real time.\n",
        "##This completes a fully functioning AI triage workflow in Colab."
      ],
      "metadata": {
        "id": "_4ozAj06mMKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"/content/support-triage-agent\", 'zip', \"/content/support-triage-agent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ZqWGdQ1nqB2",
        "outputId": "8fa2181a-0f0e-4ba6-eed9-7b4178c84290"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/support-triage-agent.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/support-triage-agent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DwuEA6HhnvG0",
        "outputId": "a1d3309c-563a-44c2-a734-9345909ca15f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be27854c-baac-4368-b2eb-05ca2b0d97f6\", \"support-triage-agent.zip\", 16381)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}